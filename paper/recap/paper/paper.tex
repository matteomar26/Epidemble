\documentclass[a4paper, amsfonts, amssymb, amsmath, reprint, showkeys, nofootinbib, twoside, floatfix, pre,superscriptaddress]{revtex4-2}

\usepackage[utf8]{inputenc}
% \usepackage{geometry}
% \geometry{verbose,lmargin=5cm,rmargin=2cm}
\usepackage[left=23mm,right=13mm,top=35mm,columnsep=15pt]{geometry} 
\setcounter{secnumdepth}{3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage[unicode=true, pdfusetitle, bookmarks=true, bookmarksnumbered=false, bookmarksopen=false, breaklinks=false, pdfborder={0 0 1}, backref=false, colorlinks=false]{hyperref}
%\usepackage[margin=0.5cm]{subcaption}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{xcolor}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\pdfoutput=1
%
% Document outline for use when preparing LaTeX manuscripts
% for Elsevier Major Reference Works
% To run with LaTeX $2\epsilon$
%
% MJR, October 2003
%

\usepackage{makeidx}\usepackage{amsfonts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother

\begin{document}
\title{Ensemble Study for Inference in Epidemic Spreading}

\maketitle

\section{Introduction}
Epidemic inference is crucial to develop advanced contact tracing strategies in order to mitigate the spreading of an epidemic.

There are regimes in which inference is harder (high transmissibility?)

The goal of this paper is to give a quantitative study, on a simple model, on the hardness of inference tasks, depending on the epidemic's parameters, properties of the contact network, and noise in the observations.

Hardness of SI inference, connection with Steiner tree problem.

\section{Ensemble Study for Inference of Epidemic Trajectories}
\subsection{Epidemic Inference}
{\it SI model on graphs.}
We consider the Susceptible-Infected (SI) model of spreading, defined over a graph $\mathcal{G}=(V,E)$.
At time $t$ a node $i\in V$ can be in two states represented by a variable $x_i^t\in\{S,I\}$.
At each time step, an infected node can infect each of its susceptible neighbors $\partial i$ with independent probabilities $\lambda_{ij}\in[0,1]$.
The dynamic is irreversible: a given node can only undergo the transition $S\to I$. 
Therefore the trajectory in time of an individual can be parameterized by its infection time $t_i$.
We assume that a subset of the nodes initiate with an infection time $t_i=0$, i.e. $x_i^0=I$.
A realization of the SI process can be univocally expressed in terms of the independent transmission delays $s_{ij}\in\{1,2, \dots, \infty\}$, following a geometrical distribution $w_{ij}(s)=\lambda_{ij}(1-\lambda_{ij})^{s-1}$.
Once the initial condition $\{x_i^0\}_{i\in V}$ and the set of transmission delays $\{s_{ij}\}_{(ij)\in E}$ is fixed, the infection times can be uniquely determined from the set of equations:
\begin{align}
\label{eq:equation_infected_times}
	t_i=\delta_{x_i^0,S}\min_{j\in\partial i}\{t_j+s_{ji}\}
\end{align}
We assume that each individual has a probability $\gamma$ to be infected at time $t=0$, and we assume for simplicity that the transmission probabilities are site-independent: $\lambda_{ij}=\lambda$ for all $(ij)\in E$.
The distribution of infection times conditioned on the realization of delays and on the initial condition can be written:
\begin{align*}
	P(\{t_i\}|\{x_i^0\},\{s_{ij}\})&=\prod_{i\in V}	\psi^*(t_i, \underline{t}_{\partial i}, x_i^0, \{s_{ji}\}_{j\in\partial i}) 
\end{align*}
where $\psi^*$ enforces the above constraint on the infection times:
\begin{align}
	\label{eq:constraint_infection_times}
	\psi^*=\mathbb{I}[t_i=\delta_{x_i^0,S}\min_{j\in\partial i}\{t_j+s_{ji}\}]
\end{align}
with $\mathbb{I}[A]$ the indicator function of the event $A$.
Once averaged over the transmission delays and over the initial condition, we obtain the following distribution of times:
\begin{align}
	\label{eq:forward_averaged}
	P(\{t_i\})&=\prod_{i\in V}\psi(t_i, \underline{t}_{\partial i})
\end{align}
where:
\begin{align*}
	\psi&=\sum_{x_i^0}\gamma(x_i^0)\sum_{\{s_{ji}\}_{j\in\partial i}}\psi^*(t_i, \underline{t}_{\partial i}, x_i^0, \{s_{ji}\}_{j\in\partial i})\prod_{j\in\partial i}w(s_{ji})\\
	&\ \text{and} \ \gamma(x) = \begin{cases}
		\gamma \ \text{if} \ x=I \\
		1-\gamma \ \text{if} \ x=S
	\end{cases}
\end{align*}

{\it Inferring individual's trajectories from partial observations.}
In the inference problem we assume that some information $\mathcal{O}=\{o_i\}_{i\in\mathcal{S}}$ on the trajectory of a subset $\mathcal{S}\subseteq V$ of individuals is given by the result of medical tests. 
Most of the time we will take $o_i=x_i^T$, i.e. we observe the state of an individual at time $t=T$.
The probability of observations $P(\mathcal{O}|\{t_i\})$ factorizes over the set of individuals:
\begin{align}
\label{eq:observations}
P(\mathcal{O}|\{t_i\})=\prod_{i\in\mathcal{S}}\rho(x_i^T|t_i) \ .
\end{align}
In the simplest case, the state of each individual at time $t=T$ is perfectly known: $\mathcal{O}=\{x_i^T\}_{i\in \mathcal{S}}$, and:
\begin{align*}
	\rho(x_i^T|t_i)=\mathbb{I}[x_i^t=r(t_i) ] \\
	{\rm with} \quad r(t)=\begin{cases}
		I & \text{if $t_i\leq T$}\\
		S & \text{if $t_i>T$}
\end{cases} \ .	
\end{align*}
One can also introduce some uncertainty in the result of medical tests, with probability $p$:
\begin{align}
	\label{eq:prob_falserate}
	\rho(x_i^T|t_i)=(1-p)\mathbb{I}[x_i^T= r(t_i)] + p\mathbb{I}[x_i^T= \overline{r(t_i)}] 
\end{align}
(with $\overline{r}$ the negation of $r$: if $r=I$ then $\overline{r}=S$). Here for simplicity we take the same value for FNR and FPR: $p_{\rm FNR}=p_{\rm FPR}=p$, but we could generalize to different FNR and FPR values.
Using Bayes rule, the posterior probability of infection times is:
\begin{align}
\label{eq:posterior}
P(\{t_i\}|\mathcal{O}) = \frac{P(\{t_i\})P(\mathcal{O}|\{t_i\})}{P(\mathcal{O})}
\end{align}
with $P(\{t_i\})$ given in~(\ref{eq:forward_averaged}).
In the Bayes optimal setting, the parameters $(\lambda,\gamma, p)$ of the true trajectory are known, this means that the parameters ($\lambda, \gamma, p$) used in the posterior probability~(\ref{eq:posterior}) are the same than the true parameters used to generate the observations.
However in real cases, the value of the parameters is not known, and we denote by ($\lambda^*, \gamma^*, p^*$) (resp. $\lambda^I, \gamma^I, p^I$) the parameters used to generate the observations (resp. to infer the infection times).

\subsection{A graphical model for the joint distribution over planted and inferred trajectories}
Our objective is to estimate how close is the time $t_i$ inferred from the posterior distribution given the observations $\mathcal{O}$, from the true infection time, that we denote $\tau_i$. 
We consider the joint distribution over the true (or planted) times $\{\tau_i\}_{i\in V}$ and the inferred times $\{t_i\}_{i\in V}$:
\begin{align}
\label{eq:joint}
\begin{aligned}
	P(\{t_i\}, \{\tau_i\}) &= P(\{\tau_i\})\sum_{\mathcal{O}}P(\mathcal{O}|\{\tau_i\})P(\{t_i\}|\mathcal{O},\{\tau_i\})\\
	&= P(\{\tau_i\})\sum_{\mathcal{O}}P(\mathcal{O}|\{\tau_i\})P(\{t_i\}|\mathcal{O})\\
	&=P(\{\tau_i\})P(\{t_i\})\sum_{\mathcal{O}}\frac{P(\mathcal{O}|\{\tau_i\})P(\mathcal{O}|\{t_i\})}{P(\mathcal{O})}
\end{aligned}
\end{align}
where in the second line we used $P(\{t_i\}|\mathcal{O},\{\tau_i\})=P(\{t_i\}|\mathcal{O})$, i.e. the probability law of $\{t_i\}$ conditioned on the observations $\{O\}$ and on the planted times $\{\tau_i\}$ depends only on the observations. In the third line we used the Bayes law~(\ref{eq:posterior}).
In the Bayes optimal setting, i.e. when $(\lambda^*, \gamma^*, p^*)=(\lambda^I, \gamma^I, p^I)$, the joint probability $P(\{t_i\}, \{\tau_i\})$ is invariant under the permutation of its two arguments $\{t_i\}, \{\tau_i\}$.

The probability distribution~(\ref{eq:joint}) cannot a priori be written as a graphical model because of the sum over the observations $\mathcal{O}$ and of the denominator $P(\mathcal{O})=\sum_{\{t_i\}}P(\{t_i\})P(\mathcal{O}|\{t_i\})$. 
We instead consider the joint probability distribution {\it conditioned} on the realization of the true initial condition $\{x_i^0\}$, on the delays $\{s_{ij}\}$, and on the the realization of the noise in the observations. 
For the last one, we introduce binary variables $c_i$, with $c_i=0$ when the observation is not corrupted ($x_i^T=r(\tau_i)$), and $c_i=1$ when the observation is corrupted: $x_i^T=\overline{r(\tau_i)}$. In this way, each $c_i$ is a Bernoulli variable of parameter $p$.
We denote $\mathcal{D}=\{\{x_i^0, c_i\}_{i\in V}, \{s_{ij},s_{ji}\}_{(ij)\in E}\}$ a realization of the disorder.
The joint probability of the planted times $\{\tau_i\}$, of the observations $\mathcal{O}=\{x_i^T\}$ and of the inferred times conditioned on the disorder is:
\begin{align*}
	P(\{t_i\},\mathcal{O},&\{\tau_i\}|\mathcal{D}) = P(\{\tau_i\}|\mathcal{D})P(\mathcal{O}, \{t_i\}|\mathcal{D},\{\tau_i\}) \\
	&=P(\{\tau_i\}|\mathcal{D})P(\mathcal{O}|\mathcal{D},\{\tau_i\})P(\{t_i\}|\mathcal{D},\{\tau_i\}, \mathcal{O})\\
	&=P(\{\tau_i\}|\mathcal{D})P(\mathcal{O}|\mathcal{D},\{\tau_i\})P(\{t_i\}|\mathcal{O})
\end{align*}
where in the last line we have again noted that the posterior distribution on the inferred times $\{t_i\}$ depends only on the observations: $P(\{t_i\}|\mathcal{D},\{\tau_i\}, \mathcal{O})=P(\{t_i\}|\mathcal{O})$.
The first term in the product is:
$$
	P(\{\tau_i\}|\mathcal{D})=\prod_{i\in V}\psi^*(\tau_i, \underline{\tau}_{\partial i};x_i^0, \{s_{ji}\}_{j\in\partial i})
$$
with $\psi^*$ given in~(\ref{eq:constraint_infection_times}).
The second term in the product is the probability of having observation $\mathcal{O}=\{x_i^T\}$ given the planted times $\{\tau_i\}$ and de disorder $\mathcal{D}$. Each $x_i^T$ is a deterministic function of $\tau_i$ and of the corruption variable $c_i$:
$$
	P(\mathcal{O}|\mathcal{D},\{\tau_i\}) = \prod_{i\in V}\mathbb{I}[x_i^T=(1-c_i)r(\tau_i)+c_i\overline{r(\tau_i)}]
$$
The third term is expressed using Baye's law:
$$
	P(\{t_i\}|\mathcal{O})=\frac{P(\{t_i\})P(\mathcal{O}|\{t_i\})}{P(\{O\})}
$$
with $P(\{t_i\})$ given in~(\ref{eq:forward_averaged}) and $P(\mathcal{O}|\{t_i\})$ given in~(\ref{eq:observations}) (with $\rho(x_i^T|\tau_i)$ given in~(\ref{eq:prob_falserate})).
Finally, the denominator $$P(\mathcal{O})=\sum_{\{t_i\}}P(\{t_i\})P(\mathcal{O}|\{t_i\})$$ can be seen as a complicated function of the observations $\mathcal{O}$, but since we have fixed the disorder $\mathcal{D}=\{\{x_i^0, c_i\}_{i\in V}, \{s_{ij},s_{ji}\}_{(ij)\in E}\}$, the observations are a deterministic function of the disorder:
$x_i^T=c_i\overline{r(\tau_i)}+(1-c_i)r(\tau_i)$, and $\tau_i$ is itself a function of the initial condition $\{x_i^0\}$ and of the delays $\{s_{ij}\}$. So we can re-write it as a normalization that depends only on the disorder:
$$
	P(\mathcal{O})=Z(\mathcal{D})
$$
We obtain a joint-probability on $\{\tau_i\}$, $\mathcal{O}$ and $\{t_i\}$ that is factorized (graphical model):
\begin{align*}
	P(\{t_i\}&,\mathcal{O},\{\tau_i\}|\mathcal{D}) = \frac{1}{Z(\mathcal{D})}\prod_{i\in V}\psi^*(\tau_i, \underline{\tau}_{\partial_i};x_i^0,\{s_{ji}\}_{j\in\partial i}) \\
	&\times \psi(t_i,\underline{t}_{\partial i})\mathbb{I}[x_i^T=(1-c_i)r(\tau_i)+c_i\overline{r(\tau_i)}] \rho(x_i^T|t_i)
\end{align*}
Summing over the observations $\mathcal{O}=\{x_i^T\}$ is harmless since only one configuration $\{x_i^T\}_{i\in V}$ is accepted due to the indicator function above ($x_i^T$ is fixed by the disorder).
We obtain the joint probability distribution of planted and inferred times $\{\tau_i\}, \{t_i\}$ conditioned on the disorder:
\begin{align}
\label{eq:joint_disorder}
\begin{aligned}
	P(\{\tau_i\}, \{t_i\}|\mathcal{D})=\frac{1}{Z(\mathcal{D})}\prod_{i\in V}&\psi^*(\tau_i, \underline{\tau}_{\partial_i};x_i^0,\{s_{ji}\}_{j\in\partial i}) \\
&\times \psi(t_i,\underline{t}_{\partial i})\xi(\tau_i, t_i;c_i)
\end{aligned}
\end{align}
with:
\begin{align}
\begin{aligned}
	\xi(\tau_i, t_i;c_i) &= \rho(x_i^T|t_i) \\
	{\rm where} \quad x_i^T&=(1-c_i)r(\tau_i)+c_i\overline{r(\tau_i)}
\end{aligned}
\end{align}
with $\rho(x|t)$ given in~(\ref{eq:prob_falserate}).

{\it The case of perfect observations.}
In that case the probability of error is zero: $p=0$ so the corrupted variables are always $c_i=0$ (no corruption), and $\rho(x_i^T|t_i)=\mathbb{I}[x_i^T=r(t_i)]$. The coupling term $\xi(\tau_i, t_i;c_i)$ between inferred and planted times in the joint probability becomes:
\begin{align*}
	\xi(\tau_i, t_i)=\mathbb{I}[r(\tau_i)=r(t_i)] \\
	{\rm where} \quad r(t)=\begin{cases}
	I & \text{if $t_i\leq T$}\\
	S & \text{if $t_i>T$}
	\end{cases} \ .
\end{align*}

\subsection{Belief-Propagation equations for the joint-probability}
The factor graph associated with the probability distribution~(\ref{eq:joint_disorder}) contains short loops which compromise the use of BP. \textcolor{blue}{\it (add a figure)} 
In order to remove these short loops, we introduce the auxiliary variables $(\tau_i^{(j)},\tau_j^{(i)},t_i^{(j)},t_j^{(i)})$ on each edge $(ij)\in E$ of the factor graph, which are the copied times $\tau_i^{(j)}=\tau_i$, and $t_i^{(j)}=t_i$ for all $j\in\partial i$.
Let $T_{ij}=(\tau_i^{(j)},\tau_j^{(i)},t_i^{(j)},t_j^{(i)})$ be the tuple gathering the copied time on edge $(ij)\in E$.
The probability distribution on these auxiliary variables is:
\begin{align}
\label{eq:prob_auxiliary}
	P(\{T_{ij}\}_{(ij)\in E}|\mathcal) &= \frac{1}{\mathcal{Z}(\mathcal{D})}\prod_{i\in V}\Psi(\{T_{il}\}_{l\in\partial i};\mathcal{D}_i) 
\end{align}
where $\mathcal{D}_i=\{\{s_{li}\}_{l\in\partial i},x_i^0, c_i\}$ is the disorder associated with vertex $i\in V$, and with:
\begin{widetext}
\begin{align}
	\Psi(\{T_{il}\}_{l\in\partial i};\mathcal{D}_i) = \xi(\tau_i^{(j)},t_i^{(j)};c_i)\psi^*(\tau_i^{(j)},\underline{\tau}_{\partial i}^{(i)};\{s_{li}\}_{l\in\partial i},x_i^0)\psi(t_i^{(j)},\underline{t}_{\partial i}^{(i)})\prod_{l\in\partial i\setminus j}\delta_{t_i^{(j)},t_i^{(l)}}\delta_{\tau_i^{(j)},\tau_i^{(l)}}
\end{align}
\end{widetext}
where $j\in\partial i$ is a given neighbour of $i$. The factor graph associated with this probability distribution now mirrors the original graph $\mathcal{G}=(V,E)$ of contact between individuals.
The variable vertices live on the edges $(ij)\in E$, and the factor vertices associated with the function $\Psi$ live on the original vertex set $V$. 
We introduce the Belief Propagation (BP) message $\mu_{i\to \Psi_j}$ on each edge $(ij)\in E$ as the marginal probability law of $T_{ij}$ in the amputated graph in which node $j$ gas been removed.
The set of BP messages obey a set of self-consistent equations:
\begin{widetext}	
\begin{align}
\label{eq:BP_equations}
\begin{aligned}
	\mu_{i\to \Psi_j}(T_{ij}) &= \frac{1}{z_{\Psi_i\to j}}\sum_{\{T_{il}\}_{k\in\partial i \setminus j}}\Psi(\{T_{il}\}_{l\in\partial i};\mathcal{D}_i)\prod_{k\in\partial i \setminus j}\mu_{k\to\Psi_i}(T_{ik})
\end{aligned}
\end{align}
\end{widetext}	
were $z_{\Psi_i\to j}$ is a normalization factor. 
These equations are exact when the contact graph $\mathcal{G}=(V,E)$ is a tree. In practice, the BP method is also used as a heuristic on random sparse instance.
Introducing a horizon time $\theta$, the random variable $T_{ij}=(\tau_i^{(j)},\tau_j^{(i)}, t_i^{(j)},t_j^{(i)})$ lives in a space of size $O(\theta^4)$.
We see in supplemental how to simplify the BP equations~\ref{eq:BP_equations}, and obtain a set of equivalent equations defined over modified BP messages living in a smaller space.


\section{Results in the Bayes-Optimal case}
\subsection{Check against simulations}
\subsection{Varying epidemic's parameters}
\subsection{Varying the false positive/negative rates}
And also dilution
\subsection{Varying graph properties}
Vary graph connectivity and graph ensembles (fat tail)
\section{Departing from Bayes-optimal conditions}
{\em Nishimori equalities for epidemic spreading.}
Explain that the Nishimori argument for Replica Symmetry still holds in this model.
\subsection{Hint of a RSB transition}
\section{Inferring epidemic's parameters}

\section{Conclusion}




\bibliography{../draft}

\end{document}
